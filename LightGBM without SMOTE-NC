# LightGBM without SMOTE-NC

library(lightgbm)
library(data.table)
library(Matrix)
library(ModelMetrics)
library(caret)
library(pROC)
library(SHAPforxgboost)
library(ggplot2)
library(gridExtra)
library(dplyr)

# Load data
data <- fread("Y:/Tyler Zander/NSQIP/Z4_NSQIP_2018_2021V9.csv")

# Save 'CaseID' and remove for modeling
case_id <- data$CaseID
data[, CaseID := NULL]

# Rename variables
setnames(data, 
         old = c("MORBPROB", "OSSIPATOS", "DOptoDis", "InitialORGSPCSSI", "PRWBC", "SEPSISPATOS", 
                 "MORTPROB", "PRCREAT", "OPTIME", "PRPLATE", "TOTAL_WRVU", "ASACLAS", "DOTHSYSEP"),
         new = c("Morbidity Probability", "Organ Space Infection PATOS", "Days from Operation to DC", 
                 "Postop Organ Space Infection", "Preoperative WBC", "Sepsis PATOS", 
                 "Mortality Probability", "Preoperative Creatinine", "Operative Time", 
                 "Preoperative Platelet Count", "Total WRVU", "ASA Class", "Days from Operation to Sepsis"))

# Specify variable type
numerical_features <- c("Age", "HEIGHT", "WEIGHT", "PRSODM", "PRBUN", "Preoperative Creatinine", "Preoperative WBC", 
                        "PRHCT", "Preoperative Platelet Count", "PRALBUM", "PRBILI", "PRSGOT", "PRALKPH", "PRPTT", "PRINR", 
                        "Mortality Probability", "Morbidity Probability", "Operative Time", "HtoODay", 
                        "Days from Operation to DC", "ASACLAS", "DSUPINFEC", "DWNDINFD", "DORGSPCSSI", 
                        "DDEHIS", "DOUPNEUMO", "DREINTUB", "DPULEMBOL", "DFAILWEAN", 
                        "DOPRENAFL", "DURNINFEC", "DCNSCVA", "DCDARREST", "DCDMI", 
                        "DOTHBLEED", "DOTHDVT", "DOTHSYSEP", "DOTHSESHOCK", "RETORPODAYS", 
                        "TOTAL_WRVU", "WRVU", "Additional_CPT_Count")

# Determine categorical variables
categorical_features <- setdiff(names(data), c(numerical_features, "UNPLANNEDREADMISSION1"))

# Convert categorical variables
data[, (categorical_features) := lapply(.SD, function(x) as.integer(as.factor(x))), 
     .SDcols = categorical_features]

# Convert the outcome to binary
data$UNPLANNEDREADMISSION1 <- as.numeric(as.factor(data$UNPLANNEDREADMISSION1)) - 1

# Set up data for modeling
y <- data$UNPLANNEDREADMISSION1
X <- as.matrix(data[, !"UNPLANNEDREADMISSION1", with = FALSE])

# Print class distribution
class_table <- table(y)
print("Class distribution:")
print(class_table)

# Set inner and outer folds
outer_folds <- 5
inner_folds <- 3

# Set seed
set.seed(42)

# Create outer fold
outer_fold_indices <- createFolds(y, k = outer_folds, returnTrain = TRUE)

# Metrics function
calculate_metrics <- function(predictions, actual, threshold = 0.5) {
  predicted_classes <- ifelse(predictions > threshold, 1, 0)
  
  # Create confusion matrix
  confusion_matrix <- table(
    factor(actual, levels = c(0, 1)),
    factor(predicted_classes, levels = c(0, 1))
  )
  
  # Extraction
  tp <- if(nrow(confusion_matrix) == 2 && ncol(confusion_matrix) == 2) confusion_matrix[2,2] else 0
  fp <- if(nrow(confusion_matrix) == 2 && ncol(confusion_matrix) == 2) confusion_matrix[1,2] else 0
  tn <- if(nrow(confusion_matrix) == 2 && ncol(confusion_matrix) == 2) confusion_matrix[1,1] else 0
  fn <- if(nrow(confusion_matrix) == 2 && ncol(confusion_matrix) == 2) confusion_matrix[2,1] else 0
  
  accuracy <- (tp + tn) / sum(confusion_matrix)
  specificity <- tn / (tn + fp)
  recall <- tp / (tp + fn)
  precision <- tp / (tp + fp)
  f1_score <- 2 * (precision * recall) / (precision + recall)
  auc_value <- auc(actual, predictions)
  brier_score <- mean((predictions - actual)^2)
  
  return(c(Accuracy = accuracy, 
           Specificity = specificity,
           Recall = recall,
           Precision = precision,
           F1_Score = f1_score,
           AUC = auc_value,
           Brier_Score = brier_score))
}

# Find F1-maximizing threshold
find_optimal_threshold <- function(predictions, actual) {
  thresholds <- seq(0.1, 0.9, by = 0.01)
  f1_scores <- sapply(thresholds, function(thresh) {
    pred_classes <- ifelse(predictions > thresh, 1, 0)
    conf_matrix <- table(
      factor(actual, levels = c(0, 1)),
      factor(pred_classes, levels = c(0, 1))
    )
    
    # Safe extraction
    tp <- if(nrow(conf_matrix) == 2 && ncol(conf_matrix) == 2) conf_matrix[2,2] else 0
    fp <- if(nrow(conf_matrix) == 2 && ncol(conf_matrix) == 2) conf_matrix[1,2] else 0
    fn <- if(nrow(conf_matrix) == 2 && ncol(conf_matrix) == 2) conf_matrix[2,1] else 0
    precision <- tp / (tp + fp)
    recall <- tp / (tp + fn)
    f1 <- 2 * (precision * recall) / (precision + recall)
    return(f1)
  })
  return(thresholds[which.max(f1_scores)])
}

# Platt Scaling
platt_scale <- function(predictions, actual) {
  # Fit platt scaling
  df <- data.frame(pred = predictions, actual = actual)
  platt_model <- glm(actual ~ pred, family = binomial(), data = df)
  return(platt_model)
}

apply_platt_scaling <- function(predictions, platt_model) {
  # Apply Platt scaling
  df <- data.frame(pred = predictions)
  calibrated_probs <- predict(platt_model, newdata = df, type = "response")
  return(calibrated_probs)
}

# Calibration evaluation
create_calibration_curve <- function(predictions, actual, bins = 10) {
  # Sort predictions
  sorted_data <- data.frame(pred = predictions, actual = actual)
  sorted_data <- sorted_data[order(sorted_data$pred), ]
  
  n <- length(predictions)
  bin_size <- ceiling(n / bins)
  
  calibration_data <- data.frame(
    bin = integer(),
    pred_mean = numeric(),
    actual_mean = numeric(),
    lower_bound = numeric(),
    upper_bound = numeric(),
    n_samples = integer()
  )
  
  for (i in 1:bins) {
    start_idx <- (i-1) * bin_size + 1
    end_idx <- min(i * bin_size, n)
    
    if (start_idx > n) break
    
    bin_data <- sorted_data[start_idx:end_idx, ]
    n_samples <- nrow(bin_data)
    
    if (n_samples > 0) {
      pred_mean <- mean(bin_data$pred)
      actual_mean <- mean(bin_data$actual)
      
      if (n_samples >= 5) {
        ci <- binom.test(sum(bin_data$actual), n_samples)$conf.int
        lower_bound <- ci[1]
        upper_bound <- ci[2]
      } else {
        lower_bound <- NA
        upper_bound <- NA
      }
      
      calibration_data <- rbind(
        calibration_data,
        data.frame(
          bin = i,
          pred_mean = pred_mean,
          actual_mean = actual_mean,
          lower_bound = lower_bound,
          upper_bound = upper_bound,
          n_samples = n_samples
        )
      )
    }
  }
  
  return(calibration_data)
}

# Recalibration evaluation
plot_calibration_curve <- function(original_cal_data, calibrated_cal_data, title = "Calibration Curve") {
  # Combine data for plotting
  original_cal_data$type <- "Original"
  calibrated_cal_data$type <- "Calibrated"
  combined_data <- rbind(original_cal_data, calibrated_cal_data)
  
  # Create plot
  p <- ggplot() +
    # Add reference line
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray50") +
    
    # Add curves
    geom_line(data = combined_data, 
              aes(x = pred_mean, y = actual_mean, color = type), 
              size = 1) +
    
    # Add confidence intervals
    geom_ribbon(data = combined_data, 
                aes(x = pred_mean, ymin = lower_bound, ymax = upper_bound, fill = type),
                alpha = 0.2) +
    
    # Add points
    geom_point(data = combined_data, 
               aes(x = pred_mean, y = actual_mean, color = type, size = n_samples)) +
    
    # Labels and theme
    scale_color_manual(values = c("Original" = "red", "Calibrated" = "blue")) +
    scale_fill_manual(values = c("Original" = "red", "Calibrated" = "blue")) +
    labs(
      title = title,
      x = "Predicted Probability",
      y = "Observed Frequency",
      color = "Model",
      fill = "Model",
      size = "Sample Size"
    ) +
    theme_minimal() +
    coord_equal() +  # Force 1:1
    xlim(0, 1) +
    ylim(0, 1)
  
  return(p)
}

# Calibrated predictions
get_cv_preds_lgb <- function(X_data, y_data, params, nrounds, nfolds = 5) {
  # Create folds for cross-validation
  set.seed(42)
  folds <- createFolds(y_data, k = nfolds, returnTrain = TRUE)
  
  # Store predictions
  oof_preds <- numeric(length(y_data))
  
  for (i in 1:nfolds) {
    # Get train and validation
    train_idx <- folds[[i]]
    valid_idx <- setdiff(1:length(y_data), train_idx)
    
    # Create training and validation data
    dtrain <- lgb.Dataset(data = X_data[train_idx, ], label = y_data[train_idx])
    
    # Train model
    model <- lgb.train(
      params = params,
      data = dtrain,
      nrounds = nrounds,
      verbose = 0
    )
    
    # Get predictions
    oof_preds[valid_idx] <- predict(model, X_data[valid_idx, ])
  }
  
  return(oof_preds)
}

# Hyperparameter grid
param_grid <- expand.grid(
  learning_rate = c(0.01, 0.05),
  max_depth = c(4, 6),
  num_leaves = c(30, 50),
  min_child_samples = c(50, 100),
  feature_fraction = c(0.7),
  bagging_fraction = c(0.7),
  lambda_l1 = c(0.1),
  lambda_l2 = c(1.0)
)

# Results
outer_fold_results <- list()
best_models <- list()
best_platt_models <- list()
test_predictions <- list()
test_actual <- list()
shap_values_all <- list()
shap_importance_all <- list()
original_pred_all <- list()
calibration_curves <- list()

cat("\n---------- STARTING NESTED CROSS-VALIDATION ----------\n")

# Start nested cross-validation
for (outer_fold in 1:outer_folds) {
  cat(sprintf("\n===== Outer Fold %d/%d =====\n", outer_fold, outer_folds))
  
  # Split data of outer folds
  outer_train_idx <- outer_fold_indices[[outer_fold]]
  outer_test_idx <- setdiff(1:length(y), outer_train_idx)
  
  X_outer_train <- X[outer_train_idx, ]
  y_outer_train <- y[outer_train_idx]
  X_outer_test <- X[outer_test_idx, ]
  y_outer_test <- y[outer_test_idx]
  
  # Calculate class weight for imbalance
  positive_cases <- sum(y_outer_train == 1)
  negative_cases <- sum(y_outer_train == 0)
  scale_pos_weight_val <- negative_cases / positive_cases
  
  # Create inner fold
  inner_fold_indices <- createFolds(y_outer_train, k = inner_folds, returnTrain = TRUE)
  
  # Inner cross-validation for tuning
  inner_results <- data.frame()
  
  for (param_set in 1:nrow(param_grid)) {
    cat(sprintf("\nTesting parameter set %d/%d...\n", param_set, nrow(param_grid)))
    
    current_params <- list(
      objective = "binary",
      metric = c("auc", "binary_logloss"),
      boost_from_average = TRUE,
      scale_pos_weight = scale_pos_weight_val * 1.0,
      learning_rate = param_grid$learning_rate[param_set],
      max_depth = param_grid$max_depth[param_set],
      num_leaves = param_grid$num_leaves[param_set],
      min_child_samples = param_grid$min_child_samples[param_set],
      feature_fraction = param_grid$feature_fraction[param_set],
      bagging_fraction = param_grid$bagging_fraction[param_set],
      bagging_freq = 5,
      lambda_l1 = param_grid$lambda_l1[param_set],
      lambda_l2 = param_grid$lambda_l2[param_set]
    )
    
    # Track errors
    inner_fold_metrics <- matrix(NA, nrow = inner_folds, ncol = 7)
    inner_fold_iterations <- numeric(inner_folds)
    
    for (inner_fold in 1:inner_folds) {
      # Get inner train and validation 
      inner_train_idx <- inner_fold_indices[[inner_fold]]
      inner_valid_idx <- setdiff(1:length(y_outer_train), inner_train_idx)
      
      # Create matrices for inner training and validation
      X_inner_train <- X_outer_train[inner_train_idx, ]
      y_inner_train <- y_outer_train[inner_train_idx]
      X_inner_valid <- X_outer_train[inner_valid_idx, ]
      y_inner_valid <- y_outer_train[inner_valid_idx]
      
      # Create LightGBM datasets
      dtrain_inner <- lgb.Dataset(data = X_inner_train, label = y_inner_train)
      dvalid_inner <- lgb.Dataset(data = X_inner_valid, label = y_inner_valid)
      
      # Train with early stopping
      inner_model <- lgb.train(
        params = current_params,
        data = dtrain_inner,
        nrounds = 1000,
        valids = list(train = dtrain_inner, valid = dvalid_inner),
        early_stopping_rounds = 50,
        verbose = 0
      )
      
      # Best iteration
      inner_fold_iterations[inner_fold] <- inner_model$best_iter
      
      # Validation predictions
      inner_val_preds <- predict(inner_model, X_inner_valid)
      
      # Apply Platt scaling
      inner_platt_model <- platt_scale(inner_val_preds, y_inner_valid)
      inner_val_preds_calibrated <- apply_platt_scaling(inner_val_preds, inner_platt_model)
      
      # Determine optimal threshold
      optimal_threshold <- find_optimal_threshold(inner_val_preds_calibrated, y_inner_valid)
      
      # Calculate metrics
      metrics <- calculate_metrics(inner_val_preds_calibrated, y_inner_valid, optimal_threshold)
      inner_fold_metrics[inner_fold, ] <- metrics
    }
    
    # Average inner fold metrics
    avg_metrics <- colMeans(inner_fold_metrics, na.rm = TRUE)
    avg_iterations <- round(mean(inner_fold_iterations))
    
    # Store parameters
    inner_results <- rbind(
      inner_results,
      data.frame(
        param_set = param_set,
        learning_rate = param_grid$learning_rate[param_set],
        max_depth = param_grid$max_depth[param_set],
        num_leaves = param_grid$num_leaves[param_set],
        min_child_samples = param_grid$min_child_samples[param_set],
        feature_fraction = param_grid$feature_fraction[param_set],
        bagging_fraction = param_grid$bagging_fraction[param_set],
        lambda_l1 = param_grid$lambda_l1[param_set],
        lambda_l2 = param_grid$lambda_l2[param_set],
        iterations = avg_iterations,
        accuracy = avg_metrics[1],
        specificity = avg_metrics[2],
        recall = avg_metrics[3],
        precision = avg_metrics[4],
        f1_score = avg_metrics[5],
        auc = avg_metrics[6],
        brier_score = avg_metrics[7]
      )
    )
    
    cat(sprintf("Parameter set %d - Avg AUC: %.4f, Avg F1: %.4f, Avg Iterations: %d\n", 
                param_set, avg_metrics[6], avg_metrics[5], avg_iterations))
  }
  
  # Find best parameters
  best_params_idx <- which.max(inner_results$auc)
  best_inner_params <- inner_results[best_params_idx, ]
  
  cat("\nBest parameters for outer fold:", outer_fold, "\n")
  print(best_inner_params[, 2:9])
  
  # Train final model on outer training set with best parameters
  best_params <- list(
    objective = "binary",
    metric = c("auc", "binary_logloss"),
    boost_from_average = TRUE,
    scale_pos_weight = scale_pos_weight_val * 1.0,
    learning_rate = best_inner_params$learning_rate,
    max_depth = best_inner_params$max_depth,
    num_leaves = best_inner_params$num_leaves,
    min_child_samples = best_inner_params$min_child_samples,
    feature_fraction = best_inner_params$feature_fraction,
    bagging_fraction = best_inner_params$bagging_fraction,
    bagging_freq = 5,
    lambda_l1 = best_inner_params$lambda_l1,
    lambda_l2 = best_inner_params$lambda_l2
  )
  
  # Create LightGBM datasets
  dtrain_outer <- lgb.Dataset(data = X_outer_train, label = y_outer_train)
  
  # Train final model for this outer fold
  final_model <- lgb.train(
    params = best_params,
    data = dtrain_outer,
    nrounds = best_inner_params$iterations,
    verbose = 1
  )
  
  # Cross-validated predictions on outer training set for Platt scaling
  cv_train_preds <- get_cv_preds_lgb(
    X_data = X_outer_train,
    y_data = y_outer_train,
    params = best_params,
    nrounds = best_inner_params$iterations,
    nfolds = 5
  )
  
  # Fit Platt scaling model
  platt_model <- platt_scale(cv_train_preds, y_outer_train)
  
  # Get predictions on test set
  test_preds_raw <- predict(final_model, X_outer_test)
  
  # Apply Platt scaling
  test_preds_calibrated <- apply_platt_scaling(test_preds_raw, platt_model)
  
  # Store original predictions
  original_pred_all[[outer_fold]] <- test_preds_raw
  
  # Calibration evaluation
  original_cal_data <- create_calibration_curve(test_preds_raw, y_outer_test, bins = 10)
  calibrated_cal_data <- create_calibration_curve(test_preds_calibrated, y_outer_test, bins = 10)
  
  # Plot
  cal_plot <- plot_calibration_curve(
    original_cal_data, 
    calibrated_cal_data,
    title = paste("Calibration Curve - Fold", outer_fold)
  )
  
  # Store calibration
  calibration_curves[[outer_fold]] <- list(
    original = original_cal_data,
    calibrated = calibrated_cal_data,
    plot = cal_plot
  )
  
  # Calculate optimal threshold and metrics
  optimal_threshold <- find_optimal_threshold(test_preds_calibrated, y_outer_test)
  test_metrics <- calculate_metrics(test_preds_calibrated, y_outer_test, optimal_threshold)
  
  # Store results
  best_models[[outer_fold]] <- final_model
  best_platt_models[[outer_fold]] <- platt_model
  test_predictions[[outer_fold]] <- test_preds_calibrated
  test_actual[[outer_fold]] <- y_outer_test
  outer_fold_results[[outer_fold]] <- list(
    metrics = test_metrics,
    threshold = optimal_threshold,
    best_params = best_inner_params
  )
  
  # Display test results
  cat("\nOuter Fold", outer_fold, "Test Results:\n")
  print(test_metrics)
  
  # Save calibration evaluation
  dir.create("lightgbm_nested_cv_results/plots", showWarnings = FALSE, recursive = TRUE)
  ggsave(
    filename = paste0("lightgbm_nested_cv_results/plots/calibration_curve_fold_", outer_fold, ".png"),
    plot = cal_plot,
    width = 8,
    height = 6
  )
  
  # Calculate SHAP values for each fold
  cat("\nCalculating SHAP values for fold", outer_fold, "...\n")
  
  # Ability to limit SHAP eval (set to include all observations)
  if (nrow(X_outer_test) > 10000) {
    sample_idx <- sample(1:nrow(X_outer_test), 10000)
    X_test_sample <- X_outer_test[sample_idx, ]
  } else {
    X_test_sample <- X_outer_test
  }
  
  tryCatch({
    # Calculate SHAP values
    shap_data <- shap.prep(final_model, X_train = X_test_sample)
    
    # Store SHAP data
    shap_values_all[[outer_fold]] <- shap_data
    
    # Calculate and store importance
    importance <- shap.importance(shap_data)
    shap_importance_all[[outer_fold]] <- importance
    
    # Display top features
    cat("\nTop 10 important features for fold", outer_fold, ":\n")
    print(head(importance, 10))
  }, error = function(e) {
    cat("Error calculating SHAP values:", e$message, "\n")
  })
}

# Combine all test predictions and actual values
combined_predictions <- unlist(test_predictions)
combined_actual <- unlist(test_actual)
combined_original_preds <- unlist(original_pred_all)

# Evaluate overall calibration
overall_original_cal <- create_calibration_curve(combined_original_preds, combined_actual, bins = 15)
overall_calibrated_cal <- create_calibration_curve(combined_predictions, combined_actual, bins = 15)

# Save overall calibration evaluation
overall_cal_plot <- plot_calibration_curve(
  overall_original_cal,
  overall_calibrated_cal,
  title = "Overall Calibration Curve (All Folds)"
)

ggsave(
  filename = "lightgbm_nested_cv_results/plots/overall_calibration_curve.png",
  plot = overall_cal_plot,
  width = 10,
  height = 8
)

fold_calibration_data <- data.frame()

for (fold in 1:outer_folds) {
  if (!is.null(calibration_curves[[fold]])) {
    orig_data <- calibration_curves[[fold]]$original
    orig_data$type <- "Original"
    orig_data$fold <- paste("Fold", fold)
    
    cal_data <- calibration_curves[[fold]]$calibrated
    cal_data$type <- "Calibrated"
    cal_data$fold <- paste("Fold", fold)
    
    fold_calibration_data <- rbind(fold_calibration_data, orig_data, cal_data)
  }
}

if (nrow(fold_calibration_data) > 0) {
  facet_cal_plot <- ggplot() +
    # Add reference line
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray50") +
    
    # Add curves
    geom_line(data = fold_calibration_data, 
              aes(x = pred_mean, y = actual_mean, color = type), 
              size = 1) +
    
    # Add points
    geom_point(data = fold_calibration_data, 
               aes(x = pred_mean, y = actual_mean, color = type, size = n_samples)) +
    
    # Facet by fold
    facet_wrap(~ fold, nrow = 2) +
    
    # Labels and theme
    scale_color_manual(values = c("Original" = "red", "Calibrated" = "blue")) +
    labs(
      title = "Calibration Curves by Fold",
      x = "Predicted Probability",
      y = "Observed Frequency",
      color = "Model",
      size = "Sample Size"
    ) +
    theme_minimal() +
    coord_equal() +  # Force 1:1 aspect ratio
    xlim(0, 1) +
    ylim(0, 1)
  
  ggsave(
    filename = "lightgbm_nested_cv_results/plots/all_folds_calibration_curves.png",
    plot = facet_cal_plot,
    width = 15,
    height = 10
  )
}

# Calculate final metrics
final_threshold <- find_optimal_threshold(combined_predictions, combined_actual)
final_metrics <- calculate_metrics(combined_predictions, combined_actual, final_threshold)

# Calculate average metrics across folds
avg_metrics <- matrix(0, nrow = 1, ncol = 7)
colnames(avg_metrics) <- c("Accuracy", "Specificity", "Recall", "Precision", "F1_Score", "AUC", "Brier_Score")

for (i in 1:outer_folds) {
  avg_metrics <- avg_metrics + outer_fold_results[[i]]$metrics
}
avg_metrics <- avg_metrics / outer_folds

# Compare Brier scores before and after calibration
brier_score_original <- mean((combined_original_preds - combined_actual)^2)
brier_score_calibrated <- mean((combined_predictions - combined_actual)^2)

cat("\n---------- CALIBRATION RESULTS ----------\n")
cat("Brier Score (Original):", brier_score_original, "\n")
cat("Brier Score (Calibrated):", brier_score_calibrated, "\n")
cat("Improvement:", (brier_score_original - brier_score_calibrated) / brier_score_original * 100, "%\n")

# Print final results
cat("\n---------- FINAL RESULTS ----------\n")
cat("\nAverage Metrics Across All Folds:\n")
print(avg_metrics)
cat("\nCombined Test Metrics:\n")
print(final_metrics)

# Combine SHAP importance across all folds
combined_importance <- data.frame()

for (i in 1:length(shap_importance_all)) {
  if (!is.null(shap_importance_all[[i]])) {
    importance_df <- as.data.frame(shap_importance_all[[i]])
    importance_df$fold <- i
    combined_importance <- rbind(combined_importance, importance_df)
  }
}

# Calculate average importance across folds
avg_importance <- NULL
if (nrow(combined_importance) > 0) {
  avg_importance <- aggregate(mean_abs_shap ~ variable, data = combined_importance, FUN = mean)
  avg_importance <- avg_importance[order(-avg_importance$mean_abs_shap), ]
  
  cat("\nAverage SHAP Feature Importance Across All Folds:\n")
  print(head(avg_importance, 10))
}
